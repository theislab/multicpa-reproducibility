seml:
  executable: MultiCPA/seml_sweep_icb.py
  name: mpert4
  output_dir: sweeps/logs
  conda_environment: kemals
  project_root_dir: ../

slurm:
  #max_simultaneous_jobs: 32
  experiments_per_job: 4
  sbatch_options_template: GPU
  sbatch_options:
    gres: gpu:1       # num GPUs
    mem: 80G          # memory
    cpus-per-task: 6  # num cores
    time: 0-10:00     # max time, D-HH:MM

###### BEGIN PARAMETER CONFIGURATION ######

fixed:
  training.checkpoint_freq: 100 # checkoint frequencty to save intermediate results
  training.num_epochs: 1000 # maximum epochs for training
  training.max_minutes: 150 # maximum computation time
  training.ignore_evaluation: False
  training.save_checkpoints: False
  training.save_dir: sweeps/checkpoints

  model.model_args.loss_ae: nb # loss
  model.model_args.patience: 50 # patience for early stopping
  model.model_args.decoder_activation: linear # non-linearity for doser function
  model.model_args.is_vae: True

  dataset.dataset_args.dataset_path: datasets/Papalexi21_prep.h5ad # full path to the anndata dataset
  dataset.dataset_args.perturbation_key: condition
  dataset.dataset_args.dose_key: dose_val
  dataset.dataset_args.cell_type_key: cell_type # necessary field for cell types. Fill it with a dummy variable if no celltypes present.
  dataset.dataset_args.split_key: split # necessary field for train, test, ood splits.
  dataset.dataset_args.counts_key: counts
  dataset.dataset_args.proteins_key: protein_expression
  dataset.dataset_args.raw_proteins_key: protein_expression_raw

grid:
  model.model_type:
      type: choice
      options:
        #- ComPert
        - TotalComPert
        #- PoEComPert
        #- TotalPoEComPert

random:
  samples: 2500
  model.model_args.doser_type:
    type: choice
    options:
      - linear
      - mlp
  model.model_args.hparams.adversary_depth:
    type: choice
    options:
      - 2
      - 3
      - 4
  model.model_args.hparams.adversary_steps:  # modified
    type: choice
    options:
      - 1
      - 2
      - 3
  model.model_args.hparams.adversary_width:
    type: choice
    options:
      - 64
      - 128
      - 256
  model.model_args.hparams.autoencoder_depth:
    type: choice
    options:
      - 3
      - 4
      #- 5
  model.model_args.hparams.autoencoder_width:
    type: choice
    options:
      - 256
      - 512
      - 1024
  model.model_args.hparams.batch_size:  #modified
    type: choice
    options:
      - 128
      - 256
      - 512
  model.model_args.hparams.dim:
    type: choice
    options:
      - 128
      - 256
      - 512
  model.model_args.hparams.dosers_depth:
    type: choice
    options:
      - 1
      - 2
      - 3
  model.model_args.hparams.dosers_width:
    type: choice
    options:
      - 32
      - 64
      - 128
  model.model_args.hparams.step_size_lr:
    type: choice
    options:
      - 15
      - 25
      - 45
  model.model_args.hparams.kl_annealing_frac:
    type: choice
    options:
      - 0.2
      - 0.3
      - 0.4
  model.model_args.hparams.kl_weight:
    type: choice
    options:
      - 1.0
      - 2.0
      - 5.0
      - 10.0
  model.model_args.hparams.dosers_lr:
    type: loguniform
    min: 1e-4
    max: 1e-2
  model.model_args.hparams.dosers_wd:
    type: loguniform
    min: 1e-8
    max: 1e-5
  model.model_args.hparams.autoencoder_lr:  #modified
    type: loguniform
    min: 1e-5
    max: 1e-3
  model.model_args.hparams.autoencoder_wd:  # modified
    type: loguniform
    min: 1e-6
    max: 1e-3
  model.model_args.hparams.adversary_lr:  #modified
    type: loguniform
    min: 1e-6
    max: 5e-4
  model.model_args.hparams.adversary_wd:  #modified
    type: loguniform
    min: 5e-7
    max: 5e-5
  model.model_args.hparams.reg_adversary:
    type: loguniform
    min: 1e-2
    max: 1e2
  model.model_args.hparams.penalty_adversary:
    type: loguniform
    min: 1e-2
    max: 1e1
  model.model_args.hparams.recon_weight_pro:
    type: loguniform
    min: 1e-2
    max: 1e-1
